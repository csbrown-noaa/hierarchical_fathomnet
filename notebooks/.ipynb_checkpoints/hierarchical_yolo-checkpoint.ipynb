{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764c8187-cbd4-4876-8265-8b9078e7fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "import pycocotools.coco\n",
    "import pycocowriter.coco2yolo\n",
    "import requests\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aec1b9-f9d8-444e-872d-e4325536c3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2deb8380-18f8-4e28-884a-90fb0aa3e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class v8DetectionHierarchicalLoss:\n",
    "    \"\"\"Criterion class for computing training losses for YOLOv8 object detection.\"\"\"\n",
    "\n",
    "    def __init__(self, model, tal_topk=10, hierarchy={}):  # model must be de-paralleled\n",
    "        # hiearchy should be {child_id: parent_id}\n",
    "        from ultralytics.utils.loss import TaskAlignedAssigner, BboxLoss\n",
    "        import torch.nn as nn\n",
    "        self.hierarchy = hierarchy\n",
    "        \"\"\"Initialize v8DetectionLoss with model parameters and task-aligned assignment settings.\"\"\"\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "        h = model.args  # hyperparameters\n",
    "\n",
    "        m = model.model[-1]  # Detect() module\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.hyp = h\n",
    "        self.stride = m.stride  # model strides\n",
    "        self.nc = m.nc  # number of classes\n",
    "        self.no = m.nc + m.reg_max * 4\n",
    "        self.reg_max = m.reg_max\n",
    "        self.device = device\n",
    "\n",
    "        self.use_dfl = m.reg_max > 1\n",
    "\n",
    "        self.assigner = TaskAlignedAssigner(topk=tal_topk, num_classes=self.nc, alpha=0.5, beta=6.0)\n",
    "        self.bbox_loss = BboxLoss(m.reg_max).to(device)\n",
    "        self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)\n",
    "\n",
    "    def preprocess(self, targets, batch_size, scale_tensor):\n",
    "        raise Exception(\"whatever!\")\n",
    "        \"\"\"Preprocess targets by converting to tensor format and scaling coordinates.\"\"\"\n",
    "        nl, ne = targets.shape\n",
    "        if nl == 0:\n",
    "            out = torch.zeros(batch_size, 0, ne - 1, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]  # image index\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            counts = counts.to(dtype=torch.int32)\n",
    "            out = torch.zeros(batch_size, counts.max(), ne - 1, device=self.device)\n",
    "            for j in range(batch_size):\n",
    "                matches = i == j\n",
    "                if n := matches.sum():\n",
    "                    out[j, :n] = targets[matches, 1:]\n",
    "            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\n",
    "        return out\n",
    "\n",
    "    def bbox_decode(self, anchor_points, pred_dist):\n",
    "        raise Exception(\"whatever!\")\n",
    "        \"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\n",
    "        if self.use_dfl:\n",
    "            b, a, c = pred_dist.shape  # batch, anchors, channels\n",
    "            pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "            # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "            # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\n",
    "        return dist2bbox(pred_dist, anchor_points, xywh=False)\n",
    "\n",
    "    def __call__(self, preds, batch):\n",
    "        \"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\"\"\"\n",
    "        ultralytics.utils.LOGGER.info(\"pickles!\")\n",
    "        loss = torch.zeros(3, device=self.device)  # box, cls, dfl\n",
    "        feats = preds[1] if isinstance(preds, tuple) else preds\n",
    "        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n",
    "            (self.reg_max * 4, self.nc), 1\n",
    "        )\n",
    "\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dtype = pred_scores.dtype\n",
    "        batch_size = pred_scores.shape[0]\n",
    "        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n",
    "        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n",
    "\n",
    "        # Targets\n",
    "        targets = torch.cat((batch[\"batch_idx\"].view(-1, 1), batch[\"cls\"].view(-1, 1), batch[\"bboxes\"]), 1)\n",
    "        targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\n",
    "        gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0.0)\n",
    "\n",
    "        # Pboxes\n",
    "        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n",
    "        # dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\n",
    "        # dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\n",
    "\n",
    "        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n",
    "            # pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\n",
    "            pred_scores.detach().sigmoid(),\n",
    "            (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n",
    "            anchor_points * stride_tensor,\n",
    "            gt_labels,\n",
    "            gt_bboxes,\n",
    "            mask_gt,\n",
    "        )\n",
    "\n",
    "        raise Exception(\"whatever!\")\n",
    "        mlflow.set_tag(\"debug_hook_called\", \"yes\")\n",
    "        mlflow.log_text(\"target shapes\", artifact_file=\"layer_output.log\")\n",
    "        mlflow.log_text(target_bboxes.shape, artifact_file=\"layer_output.log\")\n",
    "        mlflow.log_text(target_scores.shape, artifact_file=\"layer_output.log\")\n",
    "        target_scores_sum = max(target_scores.sum(), 1)\n",
    "\n",
    "        # Cls loss\n",
    "        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way        \n",
    "        \n",
    "        loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n",
    "\n",
    "        # Bbox loss\n",
    "        if fg_mask.sum():\n",
    "            target_bboxes /= stride_tensor\n",
    "            loss[0], loss[2] = self.bbox_loss(\n",
    "                pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask\n",
    "            )\n",
    "\n",
    "        loss[0] *= self.hyp.box  # box gain\n",
    "        loss[1] *= self.hyp.cls  # cls gain\n",
    "        loss[2] *= self.hyp.dfl  # dfl gain\n",
    "\n",
    "        return loss * batch_size, loss.detach()  # loss(box, cls, dfl)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96ff895c-527e-4363-8f67-e4c014c0f860",
   "metadata": {},
   "source": [
    "class v8DetectionLoss:\n",
    "    \"\"\"Criterion class for computing training losses for YOLOv8 object detection.\"\"\"\n",
    "\n",
    "    def __init__(self, model, tal_topk=10):  # model must be de-paralleled\n",
    "        from ultralytics.utils.loss import TaskAlignedAssigner, BboxLoss\n",
    "        import torch.nn as nn\n",
    "        \"\"\"Initialize v8DetectionLoss with model parameters and task-aligned assignment settings.\"\"\"\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "        h = model.args  # hyperparameters\n",
    "\n",
    "        m = model.model[-1]  # Detect() module\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.hyp = h\n",
    "        self.stride = m.stride  # model strides\n",
    "        self.nc = m.nc  # number of classes\n",
    "        self.no = m.nc + m.reg_max * 4\n",
    "        self.reg_max = m.reg_max\n",
    "        self.device = device\n",
    "\n",
    "        self.use_dfl = m.reg_max > 1\n",
    "\n",
    "        self.assigner = TaskAlignedAssigner(topk=tal_topk, num_classes=self.nc, alpha=0.5, beta=6.0)\n",
    "        self.bbox_loss = BboxLoss(m.reg_max).to(device)\n",
    "        self.proj = torch.arange(m.reg_max, dtype=torch.float, device=device)\n",
    "\n",
    "    def preprocess(self, targets, batch_size, scale_tensor):\n",
    "        \"\"\"Preprocess targets by converting to tensor format and scaling coordinates.\"\"\"\n",
    "        nl, ne = targets.shape\n",
    "        if nl == 0:\n",
    "            out = torch.zeros(batch_size, 0, ne - 1, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]  # image index\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            counts = counts.to(dtype=torch.int32)\n",
    "            out = torch.zeros(batch_size, counts.max(), ne - 1, device=self.device)\n",
    "            for j in range(batch_size):\n",
    "                matches = i == j\n",
    "                if n := matches.sum():\n",
    "                    out[j, :n] = targets[matches, 1:]\n",
    "            out[..., 1:5] = xywh2xyxy(out[..., 1:5].mul_(scale_tensor))\n",
    "        return out\n",
    "\n",
    "    def bbox_decode(self, anchor_points, pred_dist):\n",
    "        \"\"\"Decode predicted object bounding box coordinates from anchor points and distribution.\"\"\"\n",
    "        if self.use_dfl:\n",
    "            b, a, c = pred_dist.shape  # batch, anchors, channels\n",
    "            pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "            # pred_dist = pred_dist.view(b, a, c // 4, 4).transpose(2,3).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n",
    "            # pred_dist = (pred_dist.view(b, a, c // 4, 4).softmax(2) * self.proj.type(pred_dist.dtype).view(1, 1, -1, 1)).sum(2)\n",
    "        return dist2bbox(pred_dist, anchor_points, xywh=False)\n",
    "\n",
    "    def __call__(self, preds, batch):\n",
    "        \"\"\"Calculate the sum of the loss for box, cls and dfl multiplied by batch size.\"\"\"\n",
    "        ultralytics.utils.LOGGER.info(\"pickles!\")\n",
    "        loss = torch.zeros(3, device=self.device)  # box, cls, dfl\n",
    "        feats = preds[1] if isinstance(preds, tuple) else preds\n",
    "        pred_distri, pred_scores = torch.cat([xi.view(feats[0].shape[0], self.no, -1) for xi in feats], 2).split(\n",
    "            (self.reg_max * 4, self.nc), 1\n",
    "        )\n",
    "\n",
    "        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n",
    "        pred_distri = pred_distri.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dtype = pred_scores.dtype\n",
    "        batch_size = pred_scores.shape[0]\n",
    "        imgsz = torch.tensor(feats[0].shape[2:], device=self.device, dtype=dtype) * self.stride[0]  # image size (h,w)\n",
    "        anchor_points, stride_tensor = make_anchors(feats, self.stride, 0.5)\n",
    "\n",
    "        # Targets\n",
    "        targets = torch.cat((batch[\"batch_idx\"].view(-1, 1), batch[\"cls\"].view(-1, 1), batch[\"bboxes\"]), 1)\n",
    "        targets = self.preprocess(targets.to(self.device), batch_size, scale_tensor=imgsz[[1, 0, 1, 0]])\n",
    "        gt_labels, gt_bboxes = targets.split((1, 4), 2)  # cls, xyxy\n",
    "        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0.0)\n",
    "\n",
    "        # Pboxes\n",
    "        pred_bboxes = self.bbox_decode(anchor_points, pred_distri)  # xyxy, (b, h*w, 4)\n",
    "        # dfl_conf = pred_distri.view(batch_size, -1, 4, self.reg_max).detach().softmax(-1)\n",
    "        # dfl_conf = (dfl_conf.amax(-1).mean(-1) + dfl_conf.amax(-1).amin(-1)) / 2\n",
    "\n",
    "        _, target_bboxes, target_scores, fg_mask, _ = self.assigner(\n",
    "            # pred_scores.detach().sigmoid() * 0.8 + dfl_conf.unsqueeze(-1) * 0.2,\n",
    "            pred_scores.detach().sigmoid(),\n",
    "            (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype),\n",
    "            anchor_points * stride_tensor,\n",
    "            gt_labels,\n",
    "            gt_bboxes,\n",
    "            mask_gt,\n",
    "        )\n",
    "\n",
    "        target_scores_sum = max(target_scores.sum(), 1)\n",
    "\n",
    "        # Cls loss\n",
    "        # loss[1] = self.varifocal_loss(pred_scores, target_scores, target_labels) / target_scores_sum  # VFL way\n",
    "        loss[1] = self.bce(pred_scores, target_scores.to(dtype)).sum() / target_scores_sum  # BCE\n",
    "\n",
    "        # Bbox loss\n",
    "        if fg_mask.sum():\n",
    "            target_bboxes /= stride_tensor\n",
    "            loss[0], loss[2] = self.bbox_loss(\n",
    "                pred_distri, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask\n",
    "            )\n",
    "\n",
    "        loss[0] *= self.hyp.box  # box gain\n",
    "        loss[1] *= self.hyp.cls  # cls gain\n",
    "        loss[2] *= self.hyp.dfl  # dfl gain\n",
    "\n",
    "        return loss * batch_size, loss.detach()  # loss(box, cls, dfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6455e-3e9a-4ffc-a7c7-12bc0a41ddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b288bbaf-f619-4327-9199-5d0002c2d47d",
   "metadata": {},
   "source": [
    "ultralytics.utils.loss.v8DetectionLoss = v8DetectionLoss\n",
    "sys.modules['ultralytics.utils.loss'].v8DetectionLoss = v8DetectionLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523216e8-c857-48f6-ab91-8d2d15a8cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '../data'\n",
    "UPLOAD_URL = 'https://storage.googleapis.com/nmfs_odp_hq/nodd_tools/datasets/oceaneyes/annotation_number_balanced_sample/annotations.json'\n",
    "DOWNLOAD_PATH = os.path.join(DATA, 'download')\n",
    "COCO_PATH = os.path.join(DOWNLOAD_PATH, 'annotations.json')\n",
    "YOLO_PATH = os.path.join(DOWNLOAD_PATH, 'yolo_training_data')\n",
    "IMAGES_PATH = os.path.join(YOLO_PATH, 'annotations', 'images')\n",
    "os.makedirs(YOLO_PATH, exist_ok=True)\n",
    "os.makedirs(DOWNLOAD_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2205cbc8-9015-48ab-b834-f10a32e2ec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "uploaded_coco_file = requests.get(UPLOAD_URL)\n",
    "with open(COCO_PATH, 'wb') as f:\n",
    "    f.write(uploaded_coco_file.content)\n",
    "    coco = pycocotools.coco.COCO(COCO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5c50cb-5844-4ee4-aed3-2731deb2ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: 2024\n",
      "version: 0.1\n",
      "description: https://www.zooniverse.org/projects/benjamin-dot-richards/oceaneyes/about/research\n",
      "contributor: None\n",
      "url: None\n",
      "date_created: 2025-02-06T20:56:54.886937+00:00\n"
     ]
    }
   ],
   "source": [
    "coco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6358262a-4a16-494a-8836-17e2b6705056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotations /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/annotations.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 2016.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO data converted successfully.\n",
      "Results saved to /home/guest/Projects/NOAA/research/hierarchical_yolo/notebooks/coco_converted\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "downloaded 0/200 images (t=0.0s)\n",
      "downloaded 1/200 images (t=0.0s)\n",
      "downloaded 2/200 images (t=0.0s)\n",
      "downloaded 3/200 images (t=0.0s)\n",
      "downloaded 4/200 images (t=0.0s)\n",
      "downloaded 5/200 images (t=0.0s)\n",
      "downloaded 6/200 images (t=0.0s)\n",
      "downloaded 7/200 images (t=0.0s)\n",
      "downloaded 8/200 images (t=0.0s)\n",
      "downloaded 9/200 images (t=0.0s)\n",
      "downloaded 10/200 images (t=0.0s)\n",
      "downloaded 11/200 images (t=0.0s)\n",
      "downloaded 12/200 images (t=0.0s)\n",
      "downloaded 13/200 images (t=0.0s)\n",
      "downloaded 14/200 images (t=0.0s)\n",
      "downloaded 15/200 images (t=0.0s)\n",
      "downloaded 16/200 images (t=0.0s)\n",
      "downloaded 17/200 images (t=0.0s)\n",
      "downloaded 18/200 images (t=0.0s)\n",
      "downloaded 19/200 images (t=0.0s)\n",
      "downloaded 20/200 images (t=0.0s)\n",
      "downloaded 21/200 images (t=0.0s)\n",
      "downloaded 22/200 images (t=0.0s)\n",
      "downloaded 23/200 images (t=0.0s)\n",
      "downloaded 24/200 images (t=0.0s)\n",
      "downloaded 25/200 images (t=0.0s)\n",
      "downloaded 26/200 images (t=0.0s)\n",
      "downloaded 27/200 images (t=0.0s)\n",
      "downloaded 28/200 images (t=0.0s)\n",
      "downloaded 29/200 images (t=0.0s)\n",
      "downloaded 30/200 images (t=0.0s)\n",
      "downloaded 31/200 images (t=0.0s)\n",
      "downloaded 32/200 images (t=0.0s)\n",
      "downloaded 33/200 images (t=0.0s)\n",
      "downloaded 34/200 images (t=0.0s)\n",
      "downloaded 35/200 images (t=0.0s)\n",
      "downloaded 36/200 images (t=0.0s)\n",
      "downloaded 37/200 images (t=0.0s)\n",
      "downloaded 38/200 images (t=0.0s)\n",
      "downloaded 39/200 images (t=0.0s)\n",
      "downloaded 40/200 images (t=0.0s)\n",
      "downloaded 41/200 images (t=0.0s)\n",
      "downloaded 42/200 images (t=0.0s)\n",
      "downloaded 43/200 images (t=0.0s)\n",
      "downloaded 44/200 images (t=0.0s)\n",
      "downloaded 45/200 images (t=0.0s)\n",
      "downloaded 46/200 images (t=0.0s)\n",
      "downloaded 47/200 images (t=0.0s)\n",
      "downloaded 48/200 images (t=0.0s)\n",
      "downloaded 49/200 images (t=0.0s)\n",
      "downloaded 50/200 images (t=0.0s)\n",
      "downloaded 51/200 images (t=0.0s)\n",
      "downloaded 52/200 images (t=0.0s)\n",
      "downloaded 53/200 images (t=0.0s)\n",
      "downloaded 54/200 images (t=0.0s)\n",
      "downloaded 55/200 images (t=0.0s)\n",
      "downloaded 56/200 images (t=0.0s)\n",
      "downloaded 57/200 images (t=0.0s)\n",
      "downloaded 58/200 images (t=0.0s)\n",
      "downloaded 59/200 images (t=0.0s)\n",
      "downloaded 60/200 images (t=0.0s)\n",
      "downloaded 61/200 images (t=0.0s)\n",
      "downloaded 62/200 images (t=0.0s)\n",
      "downloaded 63/200 images (t=0.0s)\n",
      "downloaded 64/200 images (t=0.0s)\n",
      "downloaded 65/200 images (t=0.0s)\n",
      "downloaded 66/200 images (t=0.0s)\n",
      "downloaded 67/200 images (t=0.0s)\n",
      "downloaded 68/200 images (t=0.0s)\n",
      "downloaded 69/200 images (t=0.0s)\n",
      "downloaded 70/200 images (t=0.0s)\n",
      "downloaded 71/200 images (t=0.0s)\n",
      "downloaded 72/200 images (t=0.0s)\n",
      "downloaded 73/200 images (t=0.0s)\n",
      "downloaded 74/200 images (t=0.0s)\n",
      "downloaded 75/200 images (t=0.0s)\n",
      "downloaded 76/200 images (t=0.0s)\n",
      "downloaded 77/200 images (t=0.0s)\n",
      "downloaded 78/200 images (t=0.0s)\n",
      "downloaded 79/200 images (t=0.0s)\n",
      "downloaded 80/200 images (t=0.0s)\n",
      "downloaded 81/200 images (t=0.0s)\n",
      "downloaded 82/200 images (t=0.0s)\n",
      "downloaded 83/200 images (t=0.0s)\n",
      "downloaded 84/200 images (t=0.0s)\n",
      "downloaded 85/200 images (t=0.0s)\n",
      "downloaded 86/200 images (t=0.0s)\n",
      "downloaded 87/200 images (t=0.0s)\n",
      "downloaded 88/200 images (t=0.0s)\n",
      "downloaded 89/200 images (t=0.0s)\n",
      "downloaded 90/200 images (t=0.0s)\n",
      "downloaded 91/200 images (t=0.0s)\n",
      "downloaded 92/200 images (t=0.0s)\n",
      "downloaded 93/200 images (t=0.0s)\n",
      "downloaded 94/200 images (t=0.0s)\n",
      "downloaded 95/200 images (t=0.0s)\n",
      "downloaded 96/200 images (t=0.0s)\n",
      "downloaded 97/200 images (t=0.0s)\n",
      "downloaded 98/200 images (t=0.0s)\n",
      "downloaded 99/200 images (t=0.0s)\n",
      "downloaded 100/200 images (t=0.0s)\n",
      "downloaded 101/200 images (t=0.0s)\n",
      "downloaded 102/200 images (t=0.0s)\n",
      "downloaded 103/200 images (t=0.0s)\n",
      "downloaded 104/200 images (t=0.0s)\n",
      "downloaded 105/200 images (t=0.0s)\n",
      "downloaded 106/200 images (t=0.0s)\n",
      "downloaded 107/200 images (t=0.0s)\n",
      "downloaded 108/200 images (t=0.0s)\n",
      "downloaded 109/200 images (t=0.0s)\n",
      "downloaded 110/200 images (t=0.0s)\n",
      "downloaded 111/200 images (t=0.0s)\n",
      "downloaded 112/200 images (t=0.0s)\n",
      "downloaded 113/200 images (t=0.0s)\n",
      "downloaded 114/200 images (t=0.0s)\n",
      "downloaded 115/200 images (t=0.0s)\n",
      "downloaded 116/200 images (t=0.0s)\n",
      "downloaded 117/200 images (t=0.0s)\n",
      "downloaded 118/200 images (t=0.0s)\n",
      "downloaded 119/200 images (t=0.0s)\n",
      "downloaded 120/200 images (t=0.0s)\n",
      "downloaded 121/200 images (t=0.0s)\n",
      "downloaded 122/200 images (t=0.0s)\n",
      "downloaded 123/200 images (t=0.0s)\n",
      "downloaded 124/200 images (t=0.0s)\n",
      "downloaded 125/200 images (t=0.0s)\n",
      "downloaded 126/200 images (t=0.0s)\n",
      "downloaded 127/200 images (t=0.0s)\n",
      "downloaded 128/200 images (t=0.0s)\n",
      "downloaded 129/200 images (t=0.0s)\n",
      "downloaded 130/200 images (t=0.0s)\n",
      "downloaded 131/200 images (t=0.0s)\n",
      "downloaded 132/200 images (t=0.0s)\n",
      "downloaded 133/200 images (t=0.0s)\n",
      "downloaded 134/200 images (t=0.0s)\n",
      "downloaded 135/200 images (t=0.0s)\n",
      "downloaded 136/200 images (t=0.0s)\n",
      "downloaded 137/200 images (t=0.0s)\n",
      "downloaded 138/200 images (t=0.0s)\n",
      "downloaded 139/200 images (t=0.0s)\n",
      "downloaded 140/200 images (t=0.0s)\n",
      "downloaded 141/200 images (t=0.0s)\n",
      "downloaded 142/200 images (t=0.0s)\n",
      "downloaded 143/200 images (t=0.0s)\n",
      "downloaded 144/200 images (t=0.0s)\n",
      "downloaded 145/200 images (t=0.0s)\n",
      "downloaded 146/200 images (t=0.0s)\n",
      "downloaded 147/200 images (t=0.0s)\n",
      "downloaded 148/200 images (t=0.0s)\n",
      "downloaded 149/200 images (t=0.0s)\n",
      "downloaded 150/200 images (t=0.0s)\n",
      "downloaded 151/200 images (t=0.0s)\n",
      "downloaded 152/200 images (t=0.0s)\n",
      "downloaded 153/200 images (t=0.0s)\n",
      "downloaded 154/200 images (t=0.0s)\n",
      "downloaded 155/200 images (t=0.0s)\n",
      "downloaded 156/200 images (t=0.0s)\n",
      "downloaded 157/200 images (t=0.0s)\n",
      "downloaded 158/200 images (t=0.0s)\n",
      "downloaded 159/200 images (t=0.0s)\n",
      "downloaded 160/200 images (t=0.0s)\n",
      "downloaded 161/200 images (t=0.0s)\n",
      "downloaded 162/200 images (t=0.0s)\n",
      "downloaded 163/200 images (t=0.0s)\n",
      "downloaded 164/200 images (t=0.0s)\n",
      "downloaded 165/200 images (t=0.0s)\n",
      "downloaded 166/200 images (t=0.0s)\n",
      "downloaded 167/200 images (t=0.0s)\n",
      "downloaded 168/200 images (t=0.0s)\n",
      "downloaded 169/200 images (t=0.0s)\n",
      "downloaded 170/200 images (t=0.0s)\n",
      "downloaded 171/200 images (t=0.0s)\n",
      "downloaded 172/200 images (t=0.0s)\n",
      "downloaded 173/200 images (t=0.0s)\n",
      "downloaded 174/200 images (t=0.0s)\n",
      "downloaded 175/200 images (t=0.0s)\n",
      "downloaded 176/200 images (t=0.0s)\n",
      "downloaded 177/200 images (t=0.0s)\n",
      "downloaded 178/200 images (t=0.0s)\n",
      "downloaded 179/200 images (t=0.0s)\n",
      "downloaded 180/200 images (t=0.0s)\n",
      "downloaded 181/200 images (t=0.0s)\n",
      "downloaded 182/200 images (t=0.0s)\n",
      "downloaded 183/200 images (t=0.0s)\n",
      "downloaded 184/200 images (t=0.0s)\n",
      "downloaded 185/200 images (t=0.0s)\n",
      "downloaded 186/200 images (t=0.0s)\n",
      "downloaded 187/200 images (t=0.0s)\n",
      "downloaded 188/200 images (t=0.0s)\n",
      "downloaded 189/200 images (t=0.0s)\n",
      "downloaded 190/200 images (t=0.0s)\n",
      "downloaded 191/200 images (t=0.0s)\n",
      "downloaded 192/200 images (t=0.0s)\n",
      "downloaded 193/200 images (t=0.0s)\n",
      "downloaded 194/200 images (t=0.0s)\n",
      "downloaded 195/200 images (t=0.0s)\n",
      "downloaded 196/200 images (t=0.0s)\n",
      "downloaded 197/200 images (t=0.0s)\n",
      "downloaded 198/200 images (t=0.0s)\n",
      "downloaded 199/200 images (t=0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pycocowriter.coco2yolo.coco2yolo(DOWNLOAD_PATH, YOLO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db4e6a-4077-4fc6-ab82-5f9999738574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05e415ec-f0c2-48e2-8633-867199e3f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: the yolov8.yaml model file downloaded from Ultralytics needs manual editing for the number of classes\n",
    "YOLO_YAML = os.path.join(DATA, 'yolov8.yaml')\n",
    "YOLO_BASE_MODEL = os.path.join(DATA, 'yolov8n.pt')\n",
    "YOLO_TRAIN_YAML = os.path.join(YOLO_PATH, 'train.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3438182-1a11-466e-8fac-c30c276de0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d1985-c845-4018-8328-daf8502727fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac7f6069-bf43-40e1-ab11-450c5425e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ no model scale passed. Assuming scale='n'.\n",
      "Transferred 319/355 items from pretrained weights\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(YOLO_YAML).load(YOLO_BASE_MODEL)  # build a new model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "497663ff-1be5-41c3-a403-10076d7ed805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.criterion = v8DetectionHierarchicalLoss(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac3c7e87-6e3f-466c-b4fe-4065202b6970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.101 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.100 ðŸš€ Python-3.12.3 torch-2.6.0+cu124 CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=../data/yolov8.yaml, data=../data/download/yolo_training_data/train.yaml, epochs=5, time=None, patience=100, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=[], workers=8, project=None, name=train16, exist_ok=False, pretrained=../data/yolov8n.pt, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/guest/Projects/NOAA/research/hierarchical_yolo/runs/detect/train16\n",
      "WARNING âš ï¸ no model scale passed. Assuming scale='n'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
      "YOLOv8 summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/yolo_training_data/annotations/labels.cache... 200 images, 0 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/2\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/yolo_training_data/annotations/images/20190913_201709_20190913.203153.002.010131.jpg: ignoring corrupt image/label: negative label values [-0.00068681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/yolo_training_data/annotations/labels.cache... 200 images, 0 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/guest/Projects/NOAA/research/hierarchical_yolo/data/download/yolo_training_data/annotations/images/20190913_201709_20190913.203153.002.010131.jpg: ignoring corrupt image/label: negative label values [-0.00068681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/guest/Projects/NOAA/research/hierarchical_yolo/runs/detect/train16/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(e570986da9a846d9bc347d189583327e) to /home/guest/Projects/NOAA/research/hierarchical_yolo/runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri /home/guest/Projects/NOAA/research/hierarchical_yolo/runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/home/guest/Projects/NOAA/research/hierarchical_yolo/runs/detect/train16\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      2.595      4.295       1.58         86        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:14<00:00,  1.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:25<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        199       6757    0.00946     0.0335     0.0106    0.00348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'ultralytics.utils.loss.v8DetectionLoss'>: it's not the same object as ultralytics.utils.loss.v8DetectionLoss",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPicklingError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mYOLO_TRAIN_YAML\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:791\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    790\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:211\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:446\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[32m    445\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save \u001b[38;5;129;01mor\u001b[39;00m final_epoch:\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m         \u001b[38;5;28mself\u001b[39m.run_callbacks(\u001b[33m\"\u001b[39m\u001b[33mon_model_save\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m# Scheduler\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:537\u001b[39m, in \u001b[36mBaseTrainer.save_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;66;03m# Serialize ckpt to a byte buffer once (faster than repeated torch.save() calls)\u001b[39;00m\n\u001b[32m    536\u001b[39m buffer = io.BytesIO()\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_fitness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbest_fitness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# resume and final checkpoints derive from EMA\u001b[39;49;00m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_optimizer_state_dict_to_fp16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_args\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# save as dict\u001b[39;49;00m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_metrics\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfitness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfitness\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_results_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43misoformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mversion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlicense\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAGPL-3.0 (https://ultralytics.com/license)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://docs.ultralytics.com\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m serialized_ckpt = buffer.getvalue()  \u001b[38;5;66;03m# get the serialized content to save\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Save checkpoints\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/ultralytics/utils/patches.py:102\u001b[39m, in \u001b[36mtorch_save\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m4\u001b[39m):  \u001b[38;5;66;03m# 3 retries\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_torch_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# unable to save, possibly waiting for device to flush or antivirus scan\u001b[39;00m\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m3\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/torch/serialization.py:944\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/NOAA/research/hierarchical_yolo/.venv/lib/python3.12/site-packages/torch/serialization.py:1190\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1187\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m persistent_id(obj)\n\u001b[32m   1189\u001b[39m pickler = PyTorchPickler(data_buf, protocol=pickle_protocol)\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m \u001b[43mpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1191\u001b[39m data_value = data_buf.getvalue()\n\u001b[32m   1192\u001b[39m zip_file.write_record(\u001b[33m\"\u001b[39m\u001b[33mdata.pkl\u001b[39m\u001b[33m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[31mPicklingError\u001b[39m: Can't pickle <class 'ultralytics.utils.loss.v8DetectionLoss'>: it's not the same object as ultralytics.utils.loss.v8DetectionLoss"
     ]
    }
   ],
   "source": [
    "results = model.train(\n",
    "    data=YOLO_TRAIN_YAML, \n",
    "    epochs=5, imgsz=640, device=[], batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98783e2-d11e-41d1-83c2-a10a12bcf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_img = os.path.join(IMAGES_PATH, str(np.random.choice(os.listdir(IMAGES_PATH))))\n",
    "random_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ac518-367d-45a0-928c-84f4f63c59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LOCATION = os.path.join('..','runs','detect')\n",
    "trained_models = os.listdir(os.path.join('..','runs','detect'))\n",
    "model_numbers = map(lambda x: int(x[len('train'):]) if len(x) > len('train') else 0, trained_models)\n",
    "latest_model = 'train' + str(max(model_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a45d2a-c07d-4810-bf73-01d7bc0c1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = YOLO(\n",
    "    YOLO_YAML\n",
    ").load(os.path.join(MODEL_LOCATION, latest_model, 'weights', 'best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e0c0a-ff41-4da1-b488-1a8e4aa49dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trained_model.predict(random_img, verbose=False, device=[], stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466de62-7ec7-47e5-bad6-46d6f8726517",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = next(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1e1b7-ad91-4447-8e90-3ccab5291edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070c873-1928-4c41-be6d-c943a40364af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79949504-3e7a-404c-a85b-2b4ba982e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.save('pickles.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba6f2e-2649-4071-9d98-52278698423d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
